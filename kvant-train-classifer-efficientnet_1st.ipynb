{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":7526248,"sourceType":"datasetVersion","datasetId":4308295},{"sourceId":8133613,"sourceType":"datasetVersion","datasetId":4793447,"isSourceIdPinned":true},{"sourceId":8134851,"sourceType":"datasetVersion","datasetId":4808680},{"sourceId":6127,"sourceType":"modelInstanceVersion","modelInstanceId":4598}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Train Notebook is based on Keras 3.0 with TF\n\n\n- tensorflow-2.15.0\n- keras_cv-0.8.1\n- keras-3.0.4\n","metadata":{}},{"cell_type":"code","source":"!pip install -q /kaggle/input/kerasv3-lib-ds/keras_cv-0.8.2-py3-none-any.whl --no-deps\n\n!pip install -q /kaggle/input/kerasv3-lib-ds/tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-deps\n\n!pip install -q /kaggle/input/kerasv3-lib-ds/keras-3.0.4-py3-none-any.whl --no-deps","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:24:47.241162Z","iopub.execute_input":"2024-04-16T07:24:47.241526Z","iopub.status.idle":"2024-04-16T07:25:55.136977Z","shell.execute_reply.started":"2024-04-16T07:24:47.241496Z","shell.execute_reply":"2024-04-16T07:25:55.135662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # you can also use jax, tensorflow or torch\n\n\nimport keras_cv\nimport keras\n#from tensorflow import keras\nfrom keras import ops\nimport tensorflow as tf\n\nimport cv2\nimport pandas as pd\nimport numpy as np\nfrom glob import glob\n\nimport joblib\nimport shutil\nimport json\n\nimport matplotlib.pyplot as plt \n\n\nimport sys\nimport platform\n\n\nimport matplotlib.pyplot as plt\nimport scipy\nimport joblib\nfrom scipy.signal import spectrogram\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-16T07:25:55.139423Z","iopub.execute_input":"2024-04-16T07:25:55.139957Z","iopub.status.idle":"2024-04-16T07:26:14.037599Z","shell.execute_reply.started":"2024-04-16T07:25:55.139922Z","shell.execute_reply":"2024-04-16T07:26:14.036668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"ОС: \", platform.system())\nprint(\"Архитектура: \", platform.machine())\nprint(\"Версия Python: \", sys.version_info)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:26:14.038705Z","iopub.execute_input":"2024-04-16T07:26:14.039189Z","iopub.status.idle":"2024-04-16T07:26:14.043938Z","shell.execute_reply.started":"2024-04-16T07:26:14.039164Z","shell.execute_reply":"2024-04-16T07:26:14.043014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.__version__)\ntry:\n    print(keras.__version__)\nexcept:\n    print('Tf.Keras')\nprint(keras_cv.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:26:14.046817Z","iopub.execute_input":"2024-04-16T07:26:14.047215Z","iopub.status.idle":"2024-04-16T07:26:14.118454Z","shell.execute_reply.started":"2024-04-16T07:26:14.047182Z","shell.execute_reply":"2024-04-16T07:26:14.117575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Определите и инициализируйте TPU\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n    print('TPU active')\nexcept ValueError:\n    tpu = None\n    print('TPU non active')\n\n# Создайте стратегию распределения\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    gpus = tf.config.list_physical_devices('GPU')\n    print(gpus)\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n        except RuntimeError as e:\n            print(e)\n            \n    if len(gpus) >= 2:\n        strategy = tf.distribute.MirroredStrategy()\n    else:\n        strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n\nprint(\"Number of replicas:\", strategy.num_replicas_in_sync)\nprint(tf.config.list_physical_devices())","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:26:14.119450Z","iopub.execute_input":"2024-04-16T07:26:14.119735Z","iopub.status.idle":"2024-04-16T07:26:14.542639Z","shell.execute_reply.started":"2024-04-16T07:26:14.119712Z","shell.execute_reply":"2024-04-16T07:26:14.541603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    \n    DEV = \"TPU\" if tpu else \"GPU\"\n    if tpu:\n        CACHE=True\n        CACHE_DIR=\"\"\n    else:\n        CACHE=False\n        CACHE_DIR=\"\"\n        \n    VERSION = 1\n    DF_VERSION = 1\n    MIX = 1 # USE MIXED PRECISION\n    SEED = 42\n    IMAGE_SIZE = (128, 128) \n    BATCH_SIZE = 256* strategy.num_replicas_in_sync\n    EPOCHS = 20\n    EARLY_STOPPING = 4 # Количество эпох через которое надо остановить обучение если нет улучшения результата\n    LR_MODE = 'cos' #'exp' , 'step' - режим измененмя скорости обучения\n    \n    CLASS_NAMES = ['BrassMono-BoldItalic', \n                   'GhastlyPanicCyr', \n                   'ambidexter_regular',\n                   'GaneshaType-Regular', \n                   'AlumniSansCollegiateOne-Italic',\n                   'AlumniSansCollegiateOne-Regular', \n                   'BrassMono-Italic',\n                   'better-vcr-5.2', \n                   'ArefRuqaaInk-Bold', \n                   'Aguante-Regular',\n                   'BrassMono-Bold', \n                   'ArefRuqaaInk-Regular', \n                   'Realest-Extended',\n                   'BrassMono-Regular', \n                   'TanaUncialSP']\n    LABEL2NAME = dict(enumerate(CLASS_NAMES))\n    NAME2LABEL = {v:k for k, v in LABEL2NAME.items()}\n    NUM_CLASSES = len(CLASS_NAMES)\n    AUTOTUNE = tf.data.AUTOTUNE\n    PRESET = 'efficientnetv2_b2_imagenet'   #'efficientnetv2_b2_imagenet','efficientnetv2_s_imagenet', 'efficientnetv2_b0_imagenet_classifier'\n                                    #\"yolo_v8_l_backbone_coco\", \"yolo_v8_m_backbone_coco\"# Name of pretrained MODEL\n                                    #\"densenet121_imagenet\"\n                                    #\"csp_darknet_l\", \"csp_darknet_l_imagenet\"\n                                    #\"vitdet_large_sa1b\"\n    fold = 2 # Which fold to set as validation data\n    VERBOSE = 1  # Verbosity\n    LR_START, LR_MAX, LR_MIN = 5e-5, 1e-4 , 1e-6\n    DATASET_DIR = '/kaggle/input/kvant-fonts-ds'\n\nCFG = Config()\nkeras.utils.set_random_seed(seed=CFG.SEED)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:26:14.545009Z","iopub.execute_input":"2024-04-16T07:26:14.545335Z","iopub.status.idle":"2024-04-16T07:26:14.555563Z","shell.execute_reply.started":"2024-04-16T07:26:14.545310Z","shell.execute_reply":"2024-04-16T07:26:14.554531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# USE MIXED PRECISION\n\nif CFG.MIX:\n    tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n    print('Mixed precision enabled')\nelse:\n    print('Using full precision')","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:26:14.556687Z","iopub.execute_input":"2024-04-16T07:26:14.557057Z","iopub.status.idle":"2024-04-16T07:26:14.572038Z","shell.execute_reply.started":"2024-04-16T07:26:14.557026Z","shell.execute_reply":"2024-04-16T07:26:14.571175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read dataset","metadata":{}},{"cell_type":"code","source":"# Исходный путь\n\nroot_dir = CFG.DATASET_DIR\ndata = []\n\n# Проходим по всем папкам в исходном каталоге\nfor font_name in os.listdir(root_dir):\n    font_dir = os.path.join(root_dir, font_name)\n    \n    # Проходим по всем папкам внутри папки шрифта\n    for image_type in os.listdir(font_dir):\n        image_dir = os.path.join(font_dir, image_type)\n        \n        # Проходим по всем файлам внутри папки типа изображения\n        for file in os.listdir(image_dir):\n            file_path = os.path.join(image_dir, file)\n            \n            # Добавляем информацию в список\n            data.append([font_name, image_type, file_path])\n\n# Создаем DataFrame\ndf = pd.DataFrame(data, columns=['font_name', 'image_type', 'path'])","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:26:14.573158Z","iopub.execute_input":"2024-04-16T07:26:14.573432Z","iopub.status.idle":"2024-04-16T07:26:45.139600Z","shell.execute_reply.started":"2024-04-16T07:26:14.573409Z","shell.execute_reply":"2024-04-16T07:26:45.138774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:26:45.140808Z","iopub.execute_input":"2024-04-16T07:26:45.141081Z","iopub.status.idle":"2024-04-16T07:26:45.161378Z","shell.execute_reply.started":"2024-04-16T07:26:45.141058Z","shell.execute_reply":"2024-04-16T07:26:45.160228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['font_name'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:26:45.165757Z","iopub.execute_input":"2024-04-16T07:26:45.166337Z","iopub.status.idle":"2024-04-16T07:26:45.232156Z","shell.execute_reply.started":"2024-04-16T07:26:45.166299Z","shell.execute_reply":"2024-04-16T07:26:45.231369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.NAME2LABEL, CFG.LABEL2NAME, CFG.NUM_CLASSES","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:26:45.233175Z","iopub.execute_input":"2024-04-16T07:26:45.233427Z","iopub.status.idle":"2024-04-16T07:26:45.521148Z","shell.execute_reply.started":"2024-04-16T07:26:45.233405Z","shell.execute_reply":"2024-04-16T07:26:45.520223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['class_label'] = df.font_name.map(CFG.NAME2LABEL)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:26:45.522214Z","iopub.execute_input":"2024-04-16T07:26:45.522516Z","iopub.status.idle":"2024-04-16T07:26:45.596679Z","shell.execute_reply.started":"2024-04-16T07:26:45.522492Z","shell.execute_reply":"2024-04-16T07:26:45.595927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class InversionLayer(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(InversionLayer, self).__init__(**kwargs)\n\n    def call(self, data, training=None):\n        if training:\n            data[\"images\"] = 1.0 - data[\"images\"]\n        return data\n    \n\ndef build_augmenter(dim=CFG.IMAGE_SIZE):\n    augmenters = [\n        #RandAugment только для 3х каналов\n        keras_cv.layers.RandAugment(value_range=(0, 1), augmentations_per_image=1, magnitude=0.1,input_shape=(None, 128, 128, 1)),\n        keras_cv.layers.RandomFlip(mode=\"horizontal_and_vertical\"),\n        #keras_cv.layers.MixUp(alpha=2.0),\n        keras_cv.layers.RandomCutout(height_factor=(0.2, 0.5), width_factor=(0.3, 0.6)), \n        keras_cv.layers.RandomShear(\n                                        x_factor=0.2,\n                                        y_factor=0.4,\n                                        interpolation=\"bilinear\",\n                                        fill_mode=\"reflect\",\n                                        fill_value=0.3,\n                                        bounding_box_format=None,\n                                        seed=CFG.SEED\n                                    ),\n        InversionLayer() # добавляем слой инверсии\n    ]\n    \n    def augment(img, label):\n        data = {\"images\":img, \"labels\":label}\n        for augmenter in augmenters:\n            if tf.random.uniform([]) < 0.2:\n                data = augmenter(data, training=True)\n        return data[\"images\"], data[\"labels\"]\n    \n    return augment\n\n\ndef build_decoder(with_labels=True, \n                  target_size=CFG.IMAGE_SIZE, \n                  dtype=32, \n                ):\n    \n    def decode_img(path):\n        file_bytes = tf.io.read_file(path)\n        image = tf.io.decode_png(file_bytes, channels=1, dtype=tf.uint8)\n        image = tf.image.resize(image, CFG.IMAGE_SIZE, method=\"bilinear\")\n        image = tf.cast(image, tf.float32) / 255.0\n        \n        # Mono channel to 3 channels to use \"ImageNet\" weights\n        #tf.print(image.shape)\n        image = tf.tile(image, [1, 1, 3])\n        \n        return image\n\n\n        \n\n\n    def decode_label(label):\n        label = tf.one_hot(label, CFG.NUM_CLASSES)\n        label = tf.cast(label, tf.float32)\n        label = tf.reshape(label, [CFG.NUM_CLASSES])\n        return label\n    \n    def decode_with_labels(path, label=None):\n        img = decode_img(path)\n        label = decode_label(label)\n        return (img, label)\n    \n    return decode_with_labels if with_labels else decode_img\n\n\ndef build_dataset( \n                  paths, \n                  labels=None, \n                  batch_size=32, \n                  cache=True,\n                  decode_fn=None,\n                  augment_fn=None,\n                  augment=False, \n                  repeat=True, \n                  shuffle=1024, \n                  cache_dir=\"\", \n                  drop_remainder=False):\n    if cache_dir != \"\" and cache is True:\n        os.makedirs(cache_dir, exist_ok=True)\n    \n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n    \n    if augment_fn is None:\n        augment_fn = build_augmenter()\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n    slices = (paths) if labels is None else (paths, labels)\n    \n    ds = tf.data.Dataset.from_tensor_slices(slices)\n    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n    ds = ds.cache(cache_dir) if cache else ds\n    ds = ds.repeat() if repeat else ds\n    if shuffle: \n        ds = ds.shuffle(shuffle, seed=CFG.SEED)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n    ds = ds.map(augment_fn, num_parallel_calls=AUTO) if augment else ds\n    ds = ds.prefetch(AUTO)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:41:26.080491Z","iopub.execute_input":"2024-04-16T07:41:26.080845Z","iopub.status.idle":"2024-04-16T07:41:26.102607Z","shell.execute_reply.started":"2024-04-16T07:41:26.080816Z","shell.execute_reply":"2024-04-16T07:41:26.101716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 🔪 | Data Split\n\nIn the following code snippet, the data is divided into `5` folds. Note that, the `groups` argument is used to prevent any overlap of patients between the training and validation sets, thus avoiding potential **data leakage** issues. Additionally, each split is stratified based on the `class_label`, ensuring a uniform distribution of class labels in each fold.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=CFG.SEED)\n\ndf[\"fold\"] = -1\ndf.reset_index(drop=True, inplace=True)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(df, y=df[\"class_label\"])):\n    df.loc[valid_idx, \"fold\"] = fold\ndf.groupby([\"fold\", \"font_name\"])[[\"path\"]].count()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:41:26.920676Z","iopub.execute_input":"2024-04-16T07:41:26.920989Z","iopub.status.idle":"2024-04-16T07:41:27.155000Z","shell.execute_reply.started":"2024-04-16T07:41:26.920963Z","shell.execute_reply":"2024-04-16T07:41:27.154056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Создаем фигуру и оси\nfig, axs = plt.subplots(5, 1, figsize=(10, 20))\n\n# Для каждой группы строим гистограмму\nfor fold in range(5):\n    # Выбираем подмножество данных для данной группы\n    subset = df[df[\"fold\"] == fold]\n    \n    # Строим гистограмму\n    sns.countplot(data=subset, x=\"class_label\", ax=axs[fold])\n    \n    # Устанавливаем заголовок\n    axs[fold].set_title(f\"Fold {fold}\")\n    \n# Показываем график\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:41:27.281325Z","iopub.execute_input":"2024-04-16T07:41:27.281598Z","iopub.status.idle":"2024-04-16T07:41:28.808065Z","shell.execute_reply.started":"2024-04-16T07:41:27.281577Z","shell.execute_reply":"2024-04-16T07:41:28.807145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:41:28.809623Z","iopub.execute_input":"2024-04-16T07:41:28.809923Z","iopub.status.idle":"2024-04-16T07:41:28.822203Z","shell.execute_reply.started":"2024-04-16T07:41:28.809898Z","shell.execute_reply":"2024-04-16T07:41:28.821330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample from  data\nsample_df = df.sample(frac=1).reset_index(drop=True)\ntrain_df = sample_df[sample_df.fold != CFG.fold]\nvalid_df = sample_df[sample_df.fold == CFG.fold]\nprint(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n\n# Train\ntrain_paths = train_df.path.values\ntrain_labels = train_df.class_label.values\n\ntrain_ds = build_dataset(paths=train_paths,  \n                         labels=train_labels,\n                         batch_size=CFG.BATCH_SIZE,\n                         repeat=True, \n                         shuffle=True, \n                         augment=False, \n                         cache=False,\n                         #cache_dir=CFG.CACHE_DIR\n                        )\n\n# Valid\nvalid_paths = valid_df.path.values\nvalid_labels = valid_df.class_label.values\n\nvalid_ds = build_dataset(paths=valid_paths,  \n                         labels=valid_labels,\n                         batch_size=CFG.BATCH_SIZE,\n                         repeat=None, \n                         shuffle=None, \n                         augment=None, \n                         cache=False,\n                         #cache_dir=CFG.CACHE_DIR\n                        )","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:44:51.274853Z","iopub.execute_input":"2024-04-16T07:44:51.275477Z","iopub.status.idle":"2024-04-16T07:44:51.760094Z","shell.execute_reply.started":"2024-04-16T07:44:51.275446Z","shell.execute_reply":"2024-04-16T07:44:51.759321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimgs, tars = next(iter(train_ds))\nprint(imgs.shape)\nSIZE = imgs.shape[1:]\nmin_value = np.min(imgs)\nmax_value = np.max(imgs)\nprint(\"Минимальное значение в матрице: \", min_value)\nprint(\"Maximal значение в матрице: \", max_value)\nnum_imgs = CFG.BATCH_SIZE\nplt.figure(figsize=(4*4, num_imgs//4*5))\nfor i in range(num_imgs):\n    plt.subplot(num_imgs//4, 4, i + 1)\n    img = imgs[i].numpy()[...,0]  # Adjust as per your image data format\n    tar = CFG.LABEL2NAME[np.argmax(tars[i].numpy())]\n    plt.imshow(img, cmap='gray')\n    plt.title(f\"Target: {tar}\")\n    plt.axis('off')\n    \nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:44:57.187044Z","iopub.execute_input":"2024-04-16T07:44:57.187942Z","iopub.status.idle":"2024-04-16T07:45:58.446981Z","shell.execute_reply.started":"2024-04-16T07:44:57.187908Z","shell.execute_reply":"2024-04-16T07:45:58.443905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 🔍 | Loss & Metric¶\n\nThe evaluation metric in this competition is KL Divergence, defined as,\n\nD\nKL\n(\nP\n∥\nQ\n)\n=\n∑\ni\n \nP\n(\ni\n)\nlog\n(\nP\n(\ni\n)\nQ\n(\ni\n)\n)\n \nWhere:\n\nP\n  is the true distribution.\nQ\n  is the predicted distribution.\nInterestingly, as KL Divergence is differentiable, we can directly use it as our loss function. Thus, we don't need to use a third-party metric like Accuracy to evaluate our model. Therefore, valid_loss can stand alone as an indicator for our evaluation. In keras, we already have impelementation for KL Divergence loss so we only need to import it.","metadata":{}},{"cell_type":"code","source":"LOSS = tf.keras.losses.KLDivergence()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:04:28.307314Z","iopub.execute_input":"2024-04-16T07:04:28.307919Z","iopub.status.idle":"2024-04-16T07:04:28.313515Z","shell.execute_reply.started":"2024-04-16T07:04:28.307885Z","shell.execute_reply":"2024-04-16T07:04:28.311659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 🤖 | Modeling¶\n\nThis notebook uses the EfficientNetV2 B2 from KerasCV's collection of pretrained models. To explore other models, simply modify the preset in the CFG (config). Check the KerasCV website for a list of available pretrained models.","metadata":{}},{"cell_type":"code","source":"def build_model():\n    # Detect hardware, return appropriate distribution strategy\n    \n    with strategy.scope():\n        # Определение входных данных\n        inputs = keras.Input(shape=(128, 128, 1))\n\n        # Слой 1\n        x = keras.layers.Conv2D(64, (3, 3), activation='relu')(inputs)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n\n        # Слой 2\n        x = keras.layers.Conv2D(128, (3, 3), activation='relu')(x)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n\n        # Слой 3\n        x = keras.layers.Conv2D(256, (3, 3), activation='relu')(x)\n        x = keras.layers.BatchNormalization()(x)\n\n        # Слой 4\n        x = keras.layers.Conv2D(256, (3, 3), activation='relu')(x)\n        x = keras.layers.BatchNormalization()(x)\n\n        # Слой 5\n        x = keras.layers.Conv2D(256, (3, 3), activation='relu')(x)\n        x = keras.layers.BatchNormalization()(x)\n        gap = keras.layers.GlobalAveragePooling2D()\n        # Полносвязные слои\n        x = gap(x)\n        x = keras.layers.Dense(4096, activation='relu')(x)\n        x = keras.layers.Dropout(0.5)(x)\n        x = keras.layers.Dense(4096, activation='relu')(x)\n        x = keras.layers.Dropout(0.5)(x)\n        outputs = keras.layers.Dense(CFG.NUM_CLASSES, activation='softmax', name= 'outputs')(x)  # Предполагается, что у вас 1000 классов\n\n        # Создание модели\n        model = keras.Model(inputs=inputs, outputs=outputs)\n\n        # Компиляция модели\n        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n        \n    return model\n\nmodel = build_model()\n# Model Sumamry\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:04:28.315163Z","iopub.execute_input":"2024-04-16T07:04:28.315943Z","iopub.status.idle":"2024-04-16T07:04:28.603961Z","shell.execute_reply.started":"2024-04-16T07:04:28.315911Z","shell.execute_reply":"2024-04-16T07:04:28.603023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    with strategy.scope():\n\n        #backbone = keras_cv.models.YOLOV8Backbone.from_preset(\"yolo_v8_xl_backbone_coco\")\n        model = keras_cv.models.ImageClassifier.from_preset(\n        \n            CFG.PRESET,\n            #pooling=\"avg\", \n            #activation=\"softmax\",\n            num_classes=CFG.NUM_CLASSES  \n        )\n\n        # Compile the model  \n        model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n                      loss='categorical_crossentropy', \n                      metrics=['accuracy']\n                     )\n    return model\nmodel = build_model()\n# Model Sumamry\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:04:28.605166Z","iopub.execute_input":"2024-04-16T07:04:28.605457Z","iopub.status.idle":"2024-04-16T07:04:36.697613Z","shell.execute_reply.started":"2024-04-16T07:04:28.605431Z","shell.execute_reply":"2024-04-16T07:04:36.696648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Early stopping round","metadata":{}},{"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=CFG.EARLY_STOPPING,\n    restore_best_weights=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:04:36.698985Z","iopub.execute_input":"2024-04-16T07:04:36.699300Z","iopub.status.idle":"2024-04-16T07:04:36.704229Z","shell.execute_reply.started":"2024-04-16T07:04:36.699273Z","shell.execute_reply":"2024-04-16T07:04:36.703117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ⚓ | LR Schedule\nA well-structured learning rate schedule is essential for efficient model training, ensuring optimal convergence and avoiding issues such as overshooting or stagnation.\n\n- lr_ramp_ep: Это количество эпох, в течение которых скорость обучения увеличивается от lr_start до lr_max. Это помогает модели медленно “разогреваться” перед тем, как перейти к более высокой скорости обучения.\n- lr_sus_ep: Это количество эпох, в течение которых скорость обучения остается на уровне lr_max. Это позволяет модели “устояться” на определенной скорости обучения перед тем, как начать затухание.\n- lr_decay: Это коэффициент затухания, который определяет, насколько быстро скорость обучения уменьшается после “поддержания”. Значение меньше 1 означает, что скорость обучения будет уменьшаться с каждой эпохой.\n- lr_start: Это начальная скорость обучения. Это значение, с которого начинается “разогрев” скорости обучения.\n- lr_max: Это максимальная скорость обучения, которую модель может достичь в процессе “разогрева”. После достижения этого значения, скорость обучения либо остается на этом уровне в течение определенного количества эпох (lr_sus_ep), либо начинает затухать.\n- lr_min: Это минимальная скорость обучения, которую модель может достичь в процессе затухания. Это значение, к которому стремится скорость обучения после “разогрева” и “поддержания”.","metadata":{}},{"cell_type":"code","source":"import math\n\n\ndef get_lr_callback(batch_size=CFG.BATCH_SIZE, \n                    mode='cos', \n                    epochs=CFG.EPOCHS, \n                    plot=False, \n                    lr_start = CFG.LR_START, \n                    lr_max = CFG.LR_MAX, \n                    lr_min = CFG.LR_MIN):\n\n    #lr_start, lr_max, lr_min = 1e-5, 6e-5 , 1e-7 #2e-6 * batch_size\n    lr_ramp_ep, lr_sus_ep, lr_decay = epochs//5, 1, 0.1\n    \n    def lrfn(epoch):  # Learning rate update function\n        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n        elif mode == 'cos':\n            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n            phase = math.pi * decay_epoch_index / decay_total_epochs\n            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n        return lr\n\n    if plot:  # Plot lr curve if plot is True\n        plt.figure(figsize=(10, 5))\n        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n        plt.xlabel('epoch'); plt.ylabel('lr')\n        plt.title('LR Scheduler')\n        plt.show()\n\n    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:04:36.705447Z","iopub.execute_input":"2024-04-16T07:04:36.705809Z","iopub.status.idle":"2024-04-16T07:04:36.719900Z","shell.execute_reply.started":"2024-04-16T07:04:36.705783Z","shell.execute_reply":"2024-04-16T07:04:36.719005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_cb = get_lr_callback(CFG.BATCH_SIZE, mode=CFG.LR_MODE, plot=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:04:36.721170Z","iopub.execute_input":"2024-04-16T07:04:36.721597Z","iopub.status.idle":"2024-04-16T07:04:36.949434Z","shell.execute_reply.started":"2024-04-16T07:04:36.721563Z","shell.execute_reply":"2024-04-16T07:04:36.948522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 💾 | Model Checkpointing & CSV logger","metadata":{}},{"cell_type":"code","source":"ckpt_cb = keras.callbacks.ModelCheckpoint(f\"/kaggle/working/best_model.keras\",\n                                         monitor='val_loss',\n                                         save_best_only=True,\n                                         save_weights_only=False,\n                                         mode='min')","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:04:36.951094Z","iopub.execute_input":"2024-04-16T07:04:36.951456Z","iopub.status.idle":"2024-04-16T07:04:36.956171Z","shell.execute_reply.started":"2024-04-16T07:04:36.951422Z","shell.execute_reply":"2024-04-16T07:04:36.955305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_logger = keras.callbacks.CSVLogger('training_log.csv', separator=',', append=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:04:36.957345Z","iopub.execute_input":"2024-04-16T07:04:36.957767Z","iopub.status.idle":"2024-04-16T07:04:36.965642Z","shell.execute_reply.started":"2024-04-16T07:04:36.957736Z","shell.execute_reply":"2024-04-16T07:04:36.964753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    history = model.fit(\n        train_ds, \n        epochs=CFG.EPOCHS,\n        callbacks=[\n            lr_cb, \n            ckpt_cb, \n            csv_logger,\n            early_stopping\n        ], \n        steps_per_epoch=len(train_df)//CFG.BATCH_SIZE,\n        validation_data=valid_ds, \n        verbose=CFG.VERBOSE\n    )","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:04:36.966884Z","iopub.execute_input":"2024-04-16T07:04:36.967433Z","iopub.status.idle":"2024-04-16T07:13:13.893904Z","shell.execute_reply.started":"2024-04-16T07:04:36.967394Z","shell.execute_reply":"2024-04-16T07:13:13.892190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# |Results of train","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"], label=\"loss\")\nplt.plot(history.history[\"val_loss\"], label=\"val loss\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:13:13.895283Z","iopub.status.idle":"2024-04-16T07:13:13.896345Z","shell.execute_reply.started":"2024-04-16T07:13:13.896089Z","shell.execute_reply":"2024-04-16T07:13:13.896111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_epoch = np.argmin(history.history['val_loss'])\nbest_loss = history.history['val_loss'][best_epoch]\nbest_acc = history.history['val_accuracy'][best_epoch]\n\nprint(f'>>>> BEST Loss  : {best_loss:.3f}\\n>>>> BEST Acc   : {best_acc:.3f}\\n>>>> BEST Epoch : {best_epoch}\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:13:13.897498Z","iopub.status.idle":"2024-04-16T07:13:13.897968Z","shell.execute_reply.started":"2024-04-16T07:13:13.897733Z","shell.execute_reply":"2024-04-16T07:13:13.897755Z"},"trusted":true},"execution_count":null,"outputs":[]}]}